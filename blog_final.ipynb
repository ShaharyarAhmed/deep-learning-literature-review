{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ab11254",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Chess has fascinated humanity for centuries, not only as a game but as a powerful medium for cognitive development. In recent years, the intersection of chess with artificial intelligence and computer vision has opened new possibilities for automated chess systems. This blog explores one such innovative approach from the paper \"[Development of an autonomous chess robot system using computer vision and deep learning](https://www.sciencedirect.com/science/article/pii/S2590123025001793)\" by Truong Duc Phuc and Bui Cao Son.\n",
    "\n",
    "The paper talks about building an autonomous chess robot which sees and understands chessboard just like a human player. This involves two main tasks\n",
    "1. Detecting the chessboard\n",
    "2. Recognizing the chess pieces\n",
    "\n",
    "## Chessboard detection:\n",
    "\n",
    "![Pipeline from the Paper](image1.png)\n",
    "\n",
    "### Canny Edge detection: (Finding the Borders of Squares)\n",
    "- It takes the chessboard image as input and converts it into grayscale\n",
    "- A Gaussian blur is applied to smooth the image and reduce noise\n",
    "- Then Canny algorithm is used to detect edges of the squares\n",
    " \n",
    "### Line Detection: (Finding the Grid Lines)\n",
    "- The edges detected by `Canny Edge Detection` are processed using the `Hough Line Transform` algorithm to identify straight lines\n",
    "- It identifies both vertical and horizontal grid lines that make up the board\n",
    " \n",
    "### Finding Intersection Points: (Corners of Squares)\n",
    "- It looks for where the lines intersect to find the corners of each square\n",
    "- A clustering algorithm groups nearby lines to avoid duplicates (e.g., multiple lines detected for the same edge). See Fig 7\n",
    " \n",
    "### Flattening the Chessboard: (Correcting Perspective)\n",
    "- If the camera isn’t perfectly overhead, the chessboard might look tilted or distorted\n",
    "- The Homography Transform fixes this by warping the image to make it look like a perfect top down view\n",
    "\n",
    "![Clustering of lines](image2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70807f7c-5ebd-436f-9b31-61a9bfe76e83",
   "metadata": {},
   "source": [
    "# Training and using the Deep Learning model\n",
    "\n",
    "## Training the deep learning model\n",
    "In paper, they mentioned that they generated their own dataset, manually labeled the images and data augmentation techniques such as:\n",
    "- Horizontal and vertical flipping\n",
    "- Rotation at various angles\n",
    "- Brightness and contrast variation\n",
    "- Synthetic perspective distortion\n",
    "\n",
    "The final dataset included 1,000 labeled images, split into:\n",
    "- 60% training\n",
    "- 20% validation\n",
    "- 20% testing\n",
    "\n",
    "## Model Architecture\n",
    "They used the `YOLOv8x object detection model` for this task. YOLO (You Only Look Once) is a SOTA one stage detector that is capable of identifying objects and their bounding boxes in a single forward pass.\n",
    "\n",
    "## Training Details\n",
    "The model was trained using the `Ultralytics YOLOv8` training pipeline with the following parameters:\n",
    "- Model: YOLOv8x pretrained on COCO, fine-tuned on our chess dataset\n",
    "- Epochs: 50\n",
    "- Batch Size: 16\n",
    "- Image Size: 416×416\n",
    "- Optimizer: SGD\n",
    "- Loss Function: Combined classification + objectness + bounding box loss\n",
    "\n",
    "During training, they observed that the model’s loss decreased steadily over the first 20 epochs and evened out around epoch 50. Precision and recall metrics also stabilized in the later stages.\n",
    "\n",
    "## Evaluation and Results\n",
    "The model’s performance was assessed using standard object detection metrics:\n",
    "- Intersection over Union (IoU) to measure bounding box accuracy\n",
    "- Precision and recall per class\n",
    "- Confusion matrix to analyze class-wise errors\n",
    "\n",
    "Most chess pieces were identified with high accuracy, getting IoU scores between 0.78 and 0.9, which is considered reliable for practical applications. As shown in the confusion matrix (Fig. 11), the model had minor misclassifications, mainly between:\n",
    "- Black bishop vs black king\n",
    "- Some white pieces confused with background due to low contrast\n",
    "\n",
    "![Confusion Matrix](image3.png)\n",
    "\n",
    "These could be further improved by expanding the dataset and refining the loss function for class imbalance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2f5b59",
   "metadata": {},
   "source": [
    "# Development of Intuition (Custom Dataset + Training)\n",
    "\n",
    "While the original paper uses a deep learning model for chess piece recognition, it does not provide access to the codebase or the dataset used for training. To gain a deeper understanding and to evaluate the approach independently, we designed and trained our own model from scratch.\n",
    "\n",
    "## Preparing the dataset\n",
    "We downloaded the dataset from Kaggle (\"https://www.kaggle.com/datasets/imtkaggleteam/chess-pieces-detection-image-dataset\"). This dataset has ~1300 images.\n",
    "\n",
    "We performed following actions on the dataset:\n",
    "- Remove first unused Classname from `data.yaml` file\n",
    "- Cleaned useless images\n",
    "- Converted `green and white` chessboard images to `black and white` images\n",
    "- Created new images with `_bnw` appended to the original filename in the same folder\n",
    "\n",
    "## Training our own deep learning model\n",
    "We selected `YOLOv8m`, a medium size variant of the `YOLOv8 object detection` family.\n",
    "\n",
    "To provide realistic visual conditions, we applied perspective augmentation with a low setting (`perspective=0.001`). It helped the model learn how chess pieces appear under slight 3D distortions.\n",
    "\n",
    "Training was conducted on `2× NVIDIA® T4 GPUs` on Kaggle. We fine-tuned the model over 50 epochs, employing:\n",
    "- Cosine learning rate scheduler (lr_cos) for stable convergence\n",
    "- Dropout rate of 0.4, which provided regularization and helped reduce overfitting\n",
    "\n",
    "After that, we evaluated it on a manually prepared test set. Our model achieved significantly higher accuracy and IoU scores compared to the results reported in the paper. The improvement is attributed to a better curated dataset, advanced augmentation strategies, and careful tuning of training hyperparameters.\n",
    "\n",
    "The custom implementation by us validated the effectiveness of the deep learning approach, it also deepened our understanding of the practical challenges and trade offs involved in designing a realtime object detection system for physical applications.\n",
    "\n",
    "## Integrating Chessboard Positioning via Custom Post-Processing Logic\n",
    "To move beyond raw detection and enable real chess gameplay, we extended our system with a custom post-processing pipeline that maps the detected chess pieces to exact board positions.\n",
    "\n",
    "While the original paper relied on Canny edge detection and line intersections for detecting the chessboard grid, we instead utilized the trained YOLOv8 model (best.pt) to directly predict bounding boxes around each piece in the image. This allowed us to bypass traditional edge-based board detection and operate purely from learned object locations.\n",
    "\n",
    "Once bounding boxes are detected:\n",
    "- We extract the (x, y) coordinates of each piece's center by calculating the midpoint of its bounding box.\n",
    "- To correct vertical bias (e.g., tall pieces like queens or rooks being detected slightly into the square above), we apply a +25% height offset. This ensures the label stays within the correct square.\n",
    "\n",
    "## Wrapping the Chessboard to Estimate Piece Positions\n",
    "For mapping detected pieces to actual squares (e.g., \"e4\", \"g6\"), we applied a homography-based image wrap.\n",
    "1. Chessboard Corner Detection: We used OpenCV’s advanced findChessboardCornersSB() function to accurately identify all 81 (9×9) grid intersections. This method is robust even under perspective distortion or imperfect lighting.\n",
    "2. Perspective Correction and Warp: With the chessboard corners identified, we applied a perspective transform to warp the image into an 800×800 square grid. This effectively removes any distortion, allowing us to treat the board as a regular 8×8 matrix.\n",
    "3. Transforming Detected Piece Coordinates: The YOLO-predicted (x, y) coordinates (from the original image) were converted into the warped grid using OpenCV’s cv2.perspectiveTransform. This step maps each piece's location from the original image space into the flattened board.\n",
    "4. Final Position Assignment: Based on the warped coordinates, we calculated the exact board cell (row, column) that each piece occupies. This positional labeling allowed us to map detections to standard chess notation (e.g., \"e2\": \"white pawn\"), making the system compatible with chess engines or UI overlays.\n",
    "\n",
    "![Our confusion matrix](image4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce31811-2d41-47a8-850a-70f4450ea343",
   "metadata": {},
   "source": [
    "# Creation of Dataset\n",
    "\n",
    "We downloaded the dataset from Kaggle (\"https://www.kaggle.com/datasets/imtkaggleteam/chess-pieces-detection-image-dataset\"). This dataset has ~1300 images.\n",
    "\n",
    "We performed following actions on the dataset:\n",
    "- Remove first unused Classname from `data.yaml` file\n",
    "- Cleaned useless images\n",
    "- Converted `green and white` chessboard images to `black and white` images\n",
    "- Created new images with `_bnw` appended to the original filename in the same folder\n",
    "- Modified all three folders `train`, `valid`, and `test`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8089948c-4714-42b2-b3ca-79d9a0ecff5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import uuid\n",
    "import shutil\n",
    "\n",
    "def change_to_black_and_white(image_path, output_path):\n",
    "\n",
    "    img_cv = cv2.imread(image_path)\n",
    "    hls = cv2.cvtColor(img_cv, cv2.COLOR_BGR2HLS)\n",
    "    lower_green = np.array([40, 40, 40])\n",
    "    upper_green = np.array([100, 255, 255])\n",
    "\n",
    "    hsv = cv2.cvtColor(img_cv, cv2.COLOR_BGR2HSV)\n",
    "    mask_green = cv2.inRange(hsv, lower_green, upper_green)\n",
    "    hls_modified = hls.copy()\n",
    "    hls_modified[mask_green > 0, 2] = 0\n",
    "    hls_modified[mask_green > 0, 0] = 0\n",
    "    result_bgr = cv2.cvtColor(hls_modified, cv2.COLOR_HLS2BGR)\n",
    "    result_rgb = cv2.cvtColor(result_bgr, cv2.COLOR_BGR2RGB)\n",
    "    Image.fromarray(result_rgb).save(output_path)\n",
    "\n",
    "folders=['train','valid','test']\n",
    "\n",
    "for folder in folders:\n",
    "    image_folder = f'Chess_pieces/{folder}/images'\n",
    "    txt_folder = f'Chess_pieces/{folder}/labels'\n",
    "    ann=pd.read_csv(f'Chess_pieces/{folder}/_annotations.csv')\n",
    "    files_name = [f for f in os.listdir(image_folder)]\n",
    "\n",
    "    for file_name in files_name:\n",
    "        original_image_path = os.path.join(image_folder, file_name)\n",
    "        original_txt_path = os.path.join(txt_folder, file_name.replace('.jpg', '.txt'))\n",
    "        if not os.path.exists(original_txt_path):\n",
    "            continue\n",
    "        new_name = str(uuid.uuid4())\n",
    "        new_image_path = os.path.join(image_folder, new_name+'.jpg')\n",
    "        new_txt_path = os.path.join(txt_folder, new_name+'.txt')\n",
    "\n",
    "        new_name_bnw = os.path.join(image_folder, new_name+'_bnw.jpg')\n",
    "        change_to_black_and_white(original_image_path, new_name_bnw)\n",
    "        new_name_bnw_txt = os.path.join(txt_folder, new_name+'_bnw.txt')\n",
    "    \n",
    "        os.rename(original_image_path, new_image_path)\n",
    "        os.rename(original_txt_path, new_txt_path)\n",
    "        shutil.copyfile(new_txt_path, new_name_bnw_txt)\n",
    "        ann.loc[ann['filename'] == file_name, 'filename'] = new_name+'.jpg'\n",
    "        bnw_row = ann.loc[ann['filename'] == new_name+'.jpg'].copy()\n",
    "        bnw_row['filename'] = new_name+'_bnw.jpg'\n",
    "        ann = pd.concat([ann, bnw_row], ignore_index=True)\n",
    "\n",
    "    ann.to_csv(f'Chess_pieces/{folder}/_annotations.csv', index=False)\n",
    "    print(f'Folder {folder} done')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
